# CS510: CV & DL. Summer 2023. Cera Oh. Prog 5. Exercise 2
import tensorflow as tf
import numpy as np
from keras import models
from keras import layers
import matplotlib.pyplot as plt
import random as rand
from scipy.stats import multivariate_normal

################################################
# Step 1
################################################
# Generator
gen_model = models.Sequential()
gen_model.add(tf.keras.Input(shape=(100,)))
gen_model.add(layers.Dense(256))
gen_model.add(layers.LeakyReLU())
gen_model.add(layers.BatchNormalization())
gen_model.add(layers.Dense(512))
gen_model.add(layers.LeakyReLU())
gen_model.add(layers.BatchNormalization())
gen_model.add(layers.Dense(1024))
gen_model.add(layers.LeakyReLU())
gen_model.add(layers.BatchNormalization())
gen_model.add(layers.Dense(784))  # (None, 784)
gen_model.add(layers.LeakyReLU())  # (None, 28, 28, 1)
gen_model.add(layers.Reshape((28, 28, 1), input_shape=(2, )))

gen_model.summary()

# Discriminator
disc_model = models.Sequential()
disc_model.add(tf.keras.Input(shape=(28, 28, 1)))
disc_model.add(layers.Flatten())  # (None, 784)
disc_model.add(layers.Dense(1024))
disc_model.add(layers.LeakyReLU())
disc_model.add(layers.BatchNormalization())
disc_model.add(layers.Dense(512))
disc_model.add(layers.LeakyReLU())
disc_model.add(layers.BatchNormalization())
disc_model.add(layers.Dense(256))
disc_model.add(layers.LeakyReLU())
disc_model.add(layers.BatchNormalization())
disc_model.add(layers.Dense(1, activation='sigmoid'))  # (None, 1)

disc_model.summary()

# Full GAN
disc_model.trainable = False
GAN_model = models.Sequential([tf.keras.layers.Input(
    shape=(100,)), gen_model, disc_model])
GAN_model.build(input_shape=(100,))
GAN_model.summary()

################################################
# Step 2
################################################
# Functions


def generate_vector():  # Generate random vector of size 100 sampled from a standard MVN distribution
    v = multivariate_normal.rvs(
        mean=0, cov=1, size=100, random_state=None)
    return v


# Model options
d_optimizer = tf.keras.optimizers.Adam()  # learning_rate=0.0002, beta_1=0.5
g_optimizer = tf.keras.optimizers.Adam()
dloss_fn = tf.keras.losses.BinaryCrossentropy()
gloss_fn = tf.keras.losses.BinaryCrossentropy()
batch_size = 100

# Dataset
test_data, train_data = tf.keras.datasets.fashion_mnist.load_data()

# Separate out image array from label array
test_img, test_label = test_data  # tuple
train_img, train_label = train_data  # tuple

# Pre-process by dividing each images by 255
test_img = test_img * 1/255
test_img = np.reshape(test_img, (len(test_img), 28, 28, 1))
train_img = train_img * 1/255
train_img = np.reshape(train_img, (len(train_img), 28, 28, 1))

# Make batches of data and shuffle (Ref: https://keras.io/guides/writing_a_training_loop_from_scratch/)
test_dataset = tf.data.Dataset.from_tensor_slices((test_img))
train_dataset = tf.data.Dataset.from_tensor_slices((train_img))
test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)


# (Ref: https://keras.io/guides/writing_a_training_loop_from_scratch/)
for epoch in range(400):

    # batch iteration for discriminator training
    for step, batch_train_img in enumerate(train_dataset):

        # Create random vector from a standard MVN distribution
        vectors = np.array([])
        for x in range(batch_size):
            v = generate_vector()
            vectors = np.append(vectors, v)
        vectors = np.reshape(vectors, (batch_size, 100))

        # Get fake images generated by the generator
        fake_imgs = gen_model(vectors)

        # Create labels for fake images
        fake_label = tf.zeros((batch_size, 1))
        f_label = tf.cast(fake_label, tf.float32)

        # Create labels for real images
        real_label = tf.ones((batch_size,  1))
        r_label = tf.cast(real_label, tf.float32)

        # Concatenate fake and real images
        r_imgs = tf.cast(batch_train_img, tf.float32)
        combined = tf.concat([fake_imgs, r_imgs], axis=0)
        labels = tf.concat(
            [f_label, r_label], axis=0)

        # Shuffle the images
        shuffled = list(zip(combined, labels))
        rand.shuffle(shuffled)
        combined, labels = zip(*shuffled)
        combined = tf.convert_to_tensor(combined)
        labels = tf.convert_to_tensor(labels)

        # Train discriminator
        disc_model.trainable = True

        with tf.GradientTape() as tape:
            results = disc_model(combined)
            disc_loss = dloss_fn(labels, results)
        grads = tape.gradient(disc_loss, disc_model.trainable_weights)
        d_optimizer.apply_gradients(zip(grads, disc_model.trainable_weights))

        # Freeze discriminator weights
        disc_model.trainable = False

    # batch iteration for generator training
    for step, batch_train_img in enumerate(train_dataset):

        # Create random vector from a standard MVN distribution
        vectors = np.array([])
        for x in range(batch_size):
            v = generate_vector()
            vectors = np.append(vectors, v)
        vectors = np.reshape(vectors, (batch_size, 100))

        # create labels
        trick_labels = tf.ones((batch_size, 1))

        # Grab images samples before training
        img_to_save = gen_model(vectors)

        # Train the generator
        with tf.GradientTape() as tape:
            GAN_results = GAN_model(vectors)
            GAN_loss = gloss_fn(trick_labels, GAN_results)
        GAN_grads = tape.gradient(GAN_loss, GAN_model.trainable_weights)
        g_optimizer.apply_gradients(
            zip(GAN_grads, GAN_model.trainable_weights))

        # Save images
        if epoch % 10 == 0 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(img_to_save[int(step)], cmap='gray')
            plt.title(f"Generated Epoch {str(epoch)} Step {str(step)}")
            path = f"generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(path)
            plt.close()
        elif epoch == 99 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(img_to_save[int(step)], cmap='gray')
            plt.title(f"Generated Epoch {str(epoch)} Step {str(step)}")
            path = f"generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(path)
            plt.close()
        elif epoch == 199 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(img_to_save[int(step)], cmap='gray')
            plt.title(f"Generated Epoch {str(epoch)} Step {str(step)}")
            path = f"generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(path)
            plt.close()
        elif epoch == 299 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(img_to_save[int(step)], cmap='gray')
            plt.title(f"Generated Epoch {str(epoch)} Step {str(step)}")
            path = f"generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(path)
            plt.close()
        elif epoch == 399 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(img_to_save[int(step)], cmap='gray')
            plt.title(f"Generated Epoch {str(epoch)} Step {str(step)}")
            path = f"generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(path)
            plt.close()
    print(f"Epoch {epoch} \n")
    print(f"Discriminator training loss {float(disc_loss)}\n")
    print(f"GAN training loss {float(GAN_loss)}\n")



