# CS510: CV & DL. Summer 2023. Cera Oh. Prog 5. Exercise 2 step 3
import tensorflow as tf
import numpy as np
from keras import models
from keras import layers
from keras import metrics
import matplotlib.pyplot as plt
import random as rand
from scipy.stats import multivariate_normal
import os
################################################
# Step 3: CNN-based GAN and Discriminator
################################################
# Generator
cnn_gen_model = models.Sequential()
cnn_gen_model.add(tf.keras.Input(shape=(6272,)))
cnn_gen_model.add(layers.Reshape(
    (7, 7, 128), input_shape=(6272, )))  # 28 * 28 * 8
cnn_gen_model.add(layers.Conv2DTranspose(56, 2, strides=2))
cnn_gen_model.add(layers.LeakyReLU())
cnn_gen_model.add(layers.BatchNormalization())  # (None, 14, 14, 56)
cnn_gen_model.add(layers.Conv2DTranspose(112, 2, strides=2))
cnn_gen_model.add(layers.LeakyReLU())
cnn_gen_model.add(layers.BatchNormalization())  # (None, 28, 28, 112)
cnn_gen_model.add(layers.Conv2D(1, 1))  # (None, 28, 28, 1)

cnn_gen_model.summary()

# Discriminator
cnn_disc_model = models.Sequential()
cnn_disc_model.add(tf.keras.Input(shape=(28, 28, 1)))
cnn_disc_model.add(layers.Conv2D(64, 2, strides=2))
cnn_disc_model.add(layers.LeakyReLU())
cnn_disc_model.add(layers.BatchNormalization())
cnn_disc_model.add(layers.Conv2D(32, 2, strides=2))
cnn_disc_model.add(layers.LeakyReLU())
cnn_disc_model.add(layers.BatchNormalization())
cnn_disc_model.add(layers.Flatten())
cnn_disc_model.add(layers.Dense(1, activation='sigmoid'))  # (None, 1)

cnn_disc_model.summary()

# Full GAN
cnn_disc_model.trainable = False
cnn_GAN_model = models.Sequential([tf.keras.layers.Input(
    shape=(6272,)), cnn_gen_model, cnn_disc_model])
cnn_GAN_model.build(input_shape=(6272,))
cnn_GAN_model.summary()

# Functions


def cnn_generate_vector():  # Generate random vector of size 100 sampled from a standard MVN distribution
    v = multivariate_normal.rvs(
        mean=0, cov=1, size=6272, random_state=None)
    return v


# Model options
cnn_d_optimizer = tf.keras.optimizers.Adam()  # learning_rate=0.0002, beta_1=0.5
cnn_g_optimizer = tf.keras.optimizers.Adam()
cnn_dloss_fn = tf.keras.losses.BinaryCrossentropy()
cnn_gloss_fn = tf.keras.losses.BinaryCrossentropy()
cnn_batch_size = 100

# Dataset
test_data, train_data = tf.keras.datasets.fashion_mnist.load_data()

# Separate out image array from label array
test_img, test_label = test_data  # tuple
train_img, train_label = train_data  # tuple

# Pre-process by dividing each images by 255
test_img = test_img * 1/255
test_img = np.reshape(test_img, (len(test_img), 28, 28, 1))
train_img = train_img * 1/255
train_img = np.reshape(train_img, (len(train_img), 28, 28, 1))

# Make batches of data and shuffle (Ref: https://keras.io/guides/writing_a_training_loop_from_scratch/)
test_dataset = tf.data.Dataset.from_tensor_slices((test_img))
train_dataset = tf.data.Dataset.from_tensor_slices((train_img))
test_dataset = test_dataset.shuffle(buffer_size=1024).batch(cnn_batch_size)
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(cnn_batch_size)


# (Ref: https://keras.io/guides/writing_a_training_loop_from_scratch/)
for epoch in range(100):

    # batch iteration for discriminator training
    for step, batch_train_img in enumerate(train_dataset):

        # Create random vector from a standard MVN distribution
        cnn_vectors = np.array([])
        for x in range(cnn_batch_size):
            v = cnn_generate_vector()
            cnn_vectors = np.append(cnn_vectors, v)
        cnn_vectors = np.reshape(cnn_vectors, (cnn_batch_size, 6272))

        # Get fake images generated by the generator
        cnn_fake_imgs = cnn_gen_model(cnn_vectors)

        # Create labels for fake images
        cnn_fake_label = tf.zeros((cnn_batch_size, 1))
        cnn_f_label = tf.cast(cnn_fake_label, tf.float32)

        # Create labels for real images
        cnn_real_label = tf.ones((cnn_batch_size,  1))
        cnn_r_label = tf.cast(cnn_real_label, tf.float32)

        # Concatenate fake and real images
        cnn_r_imgs = tf.cast(batch_train_img, tf.float32)
        cnn_combined = tf.concat([cnn_fake_imgs, cnn_r_imgs], axis=0)
        cnn_labels = tf.concat(
            [cnn_f_label, cnn_r_label], axis=0)

        # Shuffle the images
        cnn_shuffled = list(zip(cnn_combined, cnn_labels))
        rand.shuffle(cnn_shuffled)
        cnn_combined, cnn_labels = zip(*cnn_shuffled)
        cnn_combined = tf.convert_to_tensor(cnn_combined)
        cnn_labels = tf.convert_to_tensor(cnn_labels)

        # Train discriminator
        cnn_disc_model.trainable = True

        with tf.GradientTape() as tape:
            cnn_results = cnn_disc_model(cnn_combined)
            cnn_disc_loss = cnn_dloss_fn(cnn_labels, cnn_results)
        cnn_grads = tape.gradient(
            cnn_disc_loss, cnn_disc_model.trainable_weights)
        cnn_d_optimizer.apply_gradients(
            zip(cnn_grads, cnn_disc_model.trainable_weights))

        # Freeze discriminator weights
        cnn_disc_model.trainable = False

    # batch iteration for generator training
    for step, batch_train_img in enumerate(train_dataset):

        # Create random vector from a standard MVN distribution
        cnn_vectors = np.array([])
        for x in range(cnn_batch_size):
            v = cnn_generate_vector()
            cnn_vectors = np.append(cnn_vectors, v)
        cnn_vectors = np.reshape(cnn_vectors, (cnn_batch_size, 6272))

        # create labels
        cnn_trick_labels = tf.ones((cnn_batch_size, 1))

        # Grab images samples before training
        cnn_img_to_save = cnn_gen_model(cnn_vectors)

        # Train the generator
        with tf.GradientTape() as tape:
            cnn_GAN_results = cnn_GAN_model(cnn_vectors)
            cnn_GAN_loss = cnn_gloss_fn(cnn_trick_labels, cnn_GAN_results)
        cnn_GAN_grads = tape.gradient(
            cnn_GAN_loss, cnn_GAN_model.trainable_weights)
        cnn_g_optimizer.apply_gradients(
            zip(cnn_GAN_grads, cnn_GAN_model.trainable_weights))

        # Save images
        if epoch % 10 == 0 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(cnn_img_to_save[int(step)], cmap='gray')
            plt.title(f"CNN Generated Epoch {str(epoch)} Step {str(step)}")
            cnn_path = f"cnn_generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(cnn_path)
            plt.close()
        elif epoch == 99 and int(step) < 3:  #
            print(f"Image saved for Epoch {epoch}")
            plt.imshow(cnn_img_to_save[int(step)], cmap='gray')
            plt.title(f"CNN Generated Epoch {str(epoch)} Step {str(step)}")
            cnn_path = f"cnn_generated_img{str(epoch)}_{str(step)}.png"
            plt.savefig(cnn_path)
            plt.close()
    print(f"Epoch {epoch} \n")
    print(f"Discriminator training loss {float(cnn_disc_loss)}\n")
    print(f"GAN training loss {float(cnn_GAN_loss)}\n")
